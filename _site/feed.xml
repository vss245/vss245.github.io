<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.3.1">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-12-13T19:12:38+01:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Veronika Shamova</title><subtitle>my personal website
</subtitle><entry><title type="html">shitty future</title><link href="http://localhost:4000/blog/2022/shitty-future/" rel="alternate" type="text/html" title="shitty future" /><published>2022-01-30T01:15:02+01:00</published><updated>2022-01-30T01:15:02+01:00</updated><id>http://localhost:4000/blog/2022/shitty-future</id><content type="html" xml:base="http://localhost:4000/blog/2022/shitty-future/"><![CDATA[<p>I’m tired of the future. The Twitter account <a href="https://twitter.com/Shitty_Future">Shitty Future</a> perfectly encapsulates how I feel about the avalanche-like growth of tech (and hype).</p>

<p>Who doesn’t want a piece of artificial intelligence? It sounds so impressive. Intelligence can be somewhat loosely defined as the capacity for logic, abstract reasoning, decision-making, self-awareness and critical thinking. When you hear “artificial intelligence”, you might even imagine an artificial being possessing these qualities (joke’s on you, I guess).</p>

<h3 id="the-all-knowing-algorithm">The all-knowing algorithm</h3>

<p>What people call AI (or machine learning) can vary. Linear regression is often the first method you look at in machine learning classes. The idea is simple: you assume your data is described by a line and you learn how to draw that line so that it’s a good fit.</p>

<p>Neural networks, while more complex, are also ultimately a powerful but simple idea: instead of a line, you have layers of interconnected units with parameters. If you find the right parameters, this network can be used to predict the output for new data. In practice, one layer of units is not enough to capture the complexity of data, so more layers are added.</p>

<p>The more complicated the method is, the harder it is to interpret the model it created. When you have a giant network of units that feed signals back and forth predicting complex output (or even <a href="https://en.wikipedia.org/wiki/Generative_adversarial_network">two networks competing with each other to generate output</a>), it’s harder to tell what your model based its output on and perhaps, to trust its “judgement” - explainable AI (XAI) is a growing field working on increasing interpretability, but we are still a long way off.</p>

<p>The allure of saying “an algorithm decided this” seems to be too high, even if the people saying it do not understand the algorithm in the slightest. There is a tendency to <a href="https://link.springer.com/article/10.1007/s11023-019-09506-6">anthropomorphize AI</a> and even imagine it as an omniscient being - the website <a href="https://blog.betterimagesofai.org/">Images of AI</a> is dedicated to finding better depictions of AI in the media, ideally those that don’t include pondering robots and a disturbing abundance of the color blue.</p>

<p><img src="/assets/img/ai.png" alt="google img search results for ai" /></p>

<p><em>I’d trust that face with my self-driving car</em></p>

<h3 id="do-androids-dream-of-anything">Do androids dream of anything?</h3>

<p>A question that has plagued the field since the infamour Turing test (and possibly before) is - do algorithms actually <em>understand</em> what they do? (Aside: I will not talk about the <a href="https://en.wikipedia.org/wiki/Chinese_room">Chinese room</a> thought experiment, mostly because I find thought experiments like that annoying).</p>

<p>However, there have been interesting developments in this question. In particular, <a href="https://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/">this article by Melanie Mitchell</a> talks about GPT-3, a famous language generation model that scores suspiciously high on questions that test understanding on ambiguous sentences (example: <em>I poured water from the bottle into the cup until it was full. What was full, the bottle or the cup?</em>). A lot of knowledge and assumptions that we take for granted are simply absent from models, which in turns influences how they behave, especially with complex real-world scenarios.</p>

<h3 id="time-is-a-flat-circle">Time is a flat circle</h3>

<p>But, to come back to the less philosophical topic at hand: the humans behind the AI. There is no new thing under the sun, and if someone invented physiognomics or eugenics once, they will be unfortunately invented again.</p>

<p>In the field of AI, common sense had not deterred people from: <a href="https://venturebeat.com/2021/01/11/outlandish-stanford-facial-recognition-study-claims-there-are-links-between-facial-features-and-political-orientation/">detecting someone’s profession from their photo</a>, <a href="https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual">detecting sexual orientation from pictures</a>, <a href="https://www.huffpost.com/entry/deepfake-tool-nudify-women_n_6112d765e4b005ed49053822">digitally undressing people</a>, <a href="https://speech2face.github.io/">generating faces of people from their speech</a>, <a href="https://en.wikipedia.org/wiki/Deepfake">putting people’s faces in videos (including pretty compromising ones)</a>, <a href="https://www.bbc.com/news/world-australia-58571618">recognizing people’s ethnicity from their faces</a>, <a href="https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/">including people’s photos in databases without their consent</a>, and <a href="https://aclanthology.org/D19-1667/">predicting prison sentences from legal cases</a> (some researchers, like the author of the sexuality detection paper, even had the audacity to claim that they are exposing threats to the LGBTQ community instead of, clearly, being a threat themselves).</p>

<p>An excellent <a href="https://arxiv.org/pdf/2106.15590.pdf">paper by Birhane et al, 2021</a> identifies implicit values that are upheld within machine learning research. Perhaps unsurprisingly, “performance”, “novelty” and “generalization” are at the forefront, while “useful”, “easy to implement” and “interpretable” are somewhere in the middle. “Fairness”, “justice” and other similar concepts have been relegated to the end. While it might seem alarmist to call an entire field completely oblivious to fairness and bias, the ever-growing kaiju battle/arms race of <a href="https://www.technologyreview.com/2021/12/21/1042835/2021-was-the-year-of-monster-ai-models/">gigantic models</a> to outperform each other on some arbitrary benchmark does seem concerning.</p>

<h3 id="extra-credit">Extra credit</h3>

<p>If you want to feel even less optimistic about the future, check out <a href="https://web3isgoinggreat.com/">Web3 is doing great!</a>, an astonishing collage of incompetence, malice and stupidity of NFT and crypto-bros (the Venn diagram of people into NFTs and those who think AI will take over the world is probably a circle).</p>

<p>If you dislike a certain billionaire as much as I do, check out <a href="https://twitter.com/tdverstynen/status/1485321117065695243">a nice Twitter thread</a> taking apart his latest nonsense.</p>

<p>If you somehow haven’t heard of it by now, <a href="https://www.powerlanguage.co.uk/wordle/">Wordle</a> is a word game that’s an ad-free once-a-day relaxation oasis - you can even save the page to HTML and it will work offline.</p>]]></content><author><name></name></author><category term="miscellaneous" /><summary type="html"><![CDATA[i’m not very optimistic]]></summary></entry><entry><title type="html">cool neuroscience papers</title><link href="http://localhost:4000/blog/2021/cool-papers/" rel="alternate" type="text/html" title="cool neuroscience papers" /><published>2021-10-10T23:15:02+02:00</published><updated>2021-10-10T23:15:02+02:00</updated><id>http://localhost:4000/blog/2021/cool-papers</id><content type="html" xml:base="http://localhost:4000/blog/2021/cool-papers/"><![CDATA[<p>Reading an array of papers and essays that are outside of my current electrophysiology/connectivity/modeling bubble definitely takes away the time that I could spend feeling guilty about my code not working, but it’s still pretty fun. Here are some very good pieces of writing on three particular topics I’ve been pondering lately.</p>

<h3 id="overpromising-and-overselling-in-science">Overpromising and overselling in science</h3>

<p><a title="Andreashorn, CC BY-SA 4.0 &lt;https://creativecommons.org/licenses/by-sa/4.0&gt;, via Wikimedia Commons" href="https://commons.wikimedia.org/wiki/File:The_Human_Connectome.png"><img width="512" alt="The Human Connectome" src="https://upload.wikimedia.org/wikipedia/commons/thumb/c/cd/The_Human_Connectome.png/512px-The_Human_Connectome.png" /></a></p>

<p><a href="https://www.eneuro.org/content/8/2/ENEURO.0521-20.2021/tab-article-info">“Promisomics and the Short-Circuiting of Mind” by Gomez-Marin, 2021</a> criticizes the popularity of “-omics” approaches to mapping the brain. These approaches focus on collecting unbelievably huge amounts of data in hopes that more data leads to more understanding: projects like the <a href="https://portal.bluebrain.epfl.ch/resources/models/cell-atlas/">Blue Brain Cell Atlas</a> used supercomputers to model millions of neurons and their connections. But more often than not, these maps ignore a lot of complexities - diagramming a circuit does not take into account individual variability or dynamical contributions of memory and experience, and, even worse, are built under a faulty assumption - that knowing where every node is and how it connects to others will magically reveal how the entire machine operates. <em>“An infinite resolution map is not the territory”</em>.</p>

<p>As someone who works with connectivity data, this is a challenging read, but I am inclined to agree with it, at least in part. Similar to the infamous <a href="https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268">“Could a Neuroscientist Understand a Microprocessor?” paper</a>, the criticism of the approach is on the conceptual level - if we don’t know what process the circuit is implementing, it’s not helpful to know its wiring diagram (and the steps to take to remedy the situation are not very clear). The emphasis could be shifted from “more data, higher resolution” to developing high-level models that allow us to understand what data to gather and how to understand it best.</p>

<p>Talking about the unpleasant consequences of the scientific model as it is now, <a href="https://magazine.scienceforthepeople.org/vol24-2-dont-be-evil/from-scientists-to-salesmen/">“From Scientists to Salesmen”, an article by Jennifer Lee</a> highlights some of the worst current trends: increasing emphasis on self-promotion and marketing of your scientific “brand”, the profitability (both in money and scientific “clout”) of quick and flashy advances, competition between labs and the scramble for publications impeding genuine progress.</p>

<p>The author calls for increased collaboration within science, as well as the creation of positions that allow scientists to simply work without feeling the need to claw their way to the top while pushing out paper after paper. Academia is a precarious job, which doesn’t bode well for our quest of understanding “life, the universe and everything” - try doing that if you don’t know if you’ll have a position in 2 years.</p>

<h3 id="scientific-revolutions-and-paradigms">Scientific revolutions and paradigms</h3>

<p><a href="https://en.wikipedia.org/wiki/The_Structure_of_Scientific_Revolutions">“The Structure of Scientific Revolutions” by Thomas Kuhn</a> has been gathering dust on my shelves for about 4 years, but hooray, the day has come for me to pick it up. According to Kuhn, science is not a slow incremental process, but rather waves of scientific paradigms disrupting each other (example: Copernicus and heliocentrism). Paradigms typically change when anomalies that can’t be accounted by current theories are discovered, but also when enough people agree to change them.</p>

<p>This book was sometimes criticized for reminding people that scientists aren’t paragons of objectivity and can occasionally be human with all the resulting irrationality. The balance between eroding people’s trust in science and instilling healthy skepticism can be pretty hard.</p>

<p><img src="/assets/img/behav.png" alt="figure from Gomez-Marin et al, 2021" style="zoom:50%;" /></p>

<p style="text-align: center;font-style: italic;"> I want to hand this image out at conferences (Figure 2 from  Gomez-Marin and Ghazanfar, 2021)</p>

<p>Some examples of suggested paradigm changes: in a scathing article directed primarily at other scientists, <a href="https://www.sciencedirect.com/science/article/pii/S0896627319307901">Gomez-Marin and Ghazanfar, 2021</a> write that “bodies aren’t just brain-holding vats that passively react to the environment”, “behavior isn’t just a stimulus-response operation” and “mice are not tiny humans”, single-handedly devastating the entire cognitive neuroscience community. I don’t think this massive paradigm change will be fully in effect any time soon, but one can hope.</p>

<p>Other fields have also been getting their fair share of criticism from the inside. <a href="https://pubmed.ncbi.nlm.nih.gov/34508955/">Weiss and Shanteau, 2021</a> discuss the current reigning paradigm in decision making research and how it led to seemingly fruitful research with little to no connection to how people actually make decisions “in the wild”.</p>

<h3 id="design-and-visualization">Design and visualization</h3>

<p>This is a much less heavy topic than “oof, science can be bad”, but is probably equally important. Awkward, cramped and hard to understand visualizations are the bread-and-butter of science, but it doesn’t have to be this way. The importance of good design also borders more serious issues, such as inaccessibility of scientific findings to laypeople and bad science journalism.</p>

<p><img src="/assets/img/graph1.png" alt="graph from Corell, 2021" style="zoom:50%;" /></p>

<p style="text-align: center;font-style: italic;"> Color me convinced! (Figure from Corell, 2021)</p>

<p><a href="https://arxiv.org/abs/2109.12975">Corell, 2021</a> discusses “bullshit visualizations” that, intentionally or not, muddle, obscure or straight up change the message of the data. These types of visualizations are less prominent in science, but it’s still a good reminder to not use more visualizations than you have to and make sure you are not deceiving anyone.</p>

<p><a href="https://www.nature.com/articles/s41467-020-19160-7">Crameri et al, 2020</a> talk about how colormaps can mislead the interpretation of data (TL;DR: don’t use the rainbow colormap, it’s bad!). I wish there was more concrete and easy to use advice out there on visualizations: every time I have to make a figure, I realize that I don’t know anything about colors or how to combine them effectively. If you know any good resources (as long as they don’t insist on telling me that red is a color that indicates passion), let me know.</p>]]></content><author><name></name></author><category term="neuroscience" /><summary type="html"><![CDATA[a bunch of interesting meta-neuroscience articles]]></summary></entry><entry><title type="html">analyzing my reading statistics</title><link href="http://localhost:4000/blog/2021/my-reading-statistics/" rel="alternate" type="text/html" title="analyzing my reading statistics" /><published>2021-08-25T16:30:50+02:00</published><updated>2021-08-25T16:30:50+02:00</updated><id>http://localhost:4000/blog/2021/my-reading-statistics</id><content type="html" xml:base="http://localhost:4000/blog/2021/my-reading-statistics/"><![CDATA[<p>Tracking my data has always helped me keep up good habits. For books, I use <a href="https://goodreads.com">Goodreads</a>, despite the sometimes borderline unusable interface and a questionable color scheme.</p>

<p>I decided to export my library and dig into my reading data a little bit. I used Jupyter Notebooks and Python (mainly pandas and matplotlib). The notebook is <a href="https://github.com/vss245/books-analysis/">here</a>.</p>

<p><img src="/assets/img/dash.png" style="zoom:50%;" /></p>

<p>Interesting insights:</p>

<ul>
  <li>Some genres to explore that I haven’t read much: history, philosophy and poetry.</li>
  <li>I have a pretty clear bias in my author countries (I’m reading a bit more German-speaking authors in German now)</li>
  <li>Publication year had to be cut off, because reading some Greek tradegies messes up the axes. Still, I’m surprised at how many “modern” (e.g. post 1950s) books I read.</li>
  <li>It’s not very straightforward to find the gender of authors - would be interesting to look at the distribution as well.</li>
</ul>

<p>Some things I found and learned along the way:</p>

<ul>
  <li>Pandas has a <a href="https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.to_datetime.html">‘to_datetime’</a> method that allows for extraction of any part of the date and other such operations (file that under ‘no need to reinvent the wheel’). The downloaded data had pretty wonky reading statistics (date read was recorded correctly on the website, but not in the CSV), so I ended up not using it as much.</li>
  <li>Python has a <a href="https://docs.python-requests.org/en/master/">‘requests’</a> package that deals with APIs, it’s actually fairly simple to use. I tried to use the OpenLibrary API to find the book genre, but since it doesn’t provide it, I had to use my existing shelves.</li>
  <li>Matplotlib has a lot of good looking color <a href="https://matplotlib.org/stable/tutorials/colors/colormaps.html">schemes</a>.</li>
  <li>I used the <a href="https://www.wikidata.org/wiki/Wikidata:Main_Page">Wikidata</a> API to find the country of origin: the description of most authors had their nationality first. If it didn’t, I discarded it. I used a list of <a href="https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv">demonyms</a> to convert this to country. Kind of convoluted, but it worked.</li>
  <li>Geopandas can be used to plot geographical data - I used Counter to count up the countries, converted this dict to a data frame and then to a Geopandas data frame (side note: Geopandas did not work without its own dedicated environment)</li>
</ul>]]></content><author><name></name></author><category term="miscellaneous" /><summary type="html"><![CDATA[firing up Jupyter to dig around my own data]]></summary></entry><entry><title type="html">programming cheatsheets</title><link href="http://localhost:4000/blog/2021/programming-cheatsheets/" rel="alternate" type="text/html" title="programming cheatsheets" /><published>2021-08-01T14:01:13+02:00</published><updated>2021-08-01T14:01:13+02:00</updated><id>http://localhost:4000/blog/2021/programming-cheatsheets</id><content type="html" xml:base="http://localhost:4000/blog/2021/programming-cheatsheets/"><![CDATA[<h3 id="programming">Programming:</h3>

<p>Since I have to switch or pick up programming languages occasionally and confusing features between them is getting pretty annoying, here are some small primers on basic syntax and features that I wrote to have a quick reference:</p>

<ul>
  <li>
    <p><a href="https://github.com/vss245/programming-notes/blob/main/Python-general.md">Python</a></p>
  </li>
  <li>
    <p><a href="https://github.com/vss245/programming-notes/blob/main/R.md">R</a></p>
  </li>
  <li>
    <p><a href="https://github.com/vss245/programming-notes/blob/main/MATLAB.md">MATLAB</a> - I dislike it (some of the reasons very eloquently explained <a href="https://neuroplausible.com/matlab">here</a>), but it’s ubiqutious in neuroscience and is a necessary evil.</p>
  </li>
  <li>
    <p><a href="https://github.com/vss245/programming-notes/blob/main/SQL.md">SQL</a> - I’ve only gotten around to learning SQL queries, definitely more to add here.</p>
  </li>
</ul>

<h3 id="non-programming-language-but-computer-related-stuff">Non-programming language but computer-related stuff:</h3>

<ul>
  <li>
    <p><a href="https://github.com/vss245/programming-notes/blob/main/Shell.md">Shell</a> - I don’t use the terminal much apart from file operations and Git, although I’ve seen some people do really impressive stuff with Makefile.</p>
  </li>
  <li>
    <p><a href="https://github.com/vss245/programming-notes/blob/main/Git.md">Git</a> - basic commands that I sometimes confuse, for an excellent and in-depth Git tutorial see <a href="https://matthew-brett.github.io/curious-git/curious_intro.html">here</a>.</p>
  </li>
</ul>]]></content><author><name></name></author><category term="miscellaneous" /><summary type="html"><![CDATA[links to little cheatsheets I wrote for myself to stop googling "how to do x python" every 2 minutes]]></summary></entry><entry><title type="html">some thoughts on resting state activity in the brain</title><link href="http://localhost:4000/blog/2021/resting-state/" rel="alternate" type="text/html" title="some thoughts on resting state activity in the brain" /><published>2021-06-15T18:40:16+02:00</published><updated>2021-06-15T18:40:16+02:00</updated><id>http://localhost:4000/blog/2021/resting-state</id><content type="html" xml:base="http://localhost:4000/blog/2021/resting-state/"><![CDATA[<p>I figured I would start writing with a small post on the subject that I have been interested in lately - resting state activity in the brain (thanks to reading <a href="https://www.sciencedirect.com/science/article/pii/S2352154621000875">this</a> paper by Laumann and Snyder).</p>

<p>Some historical information first, if only to organize my thoughts: even though electrophysiology started with a discovery of spontaneous electrical activity in the brain (Hans Berger recording alpha waves all the way back in <a href="https://www.sciencenews.org/article/hans-berger-telepathy-neuroscience-brain-eeg">1924</a> or so), neuroscientists and cognitive scientists weren’t much concerned with what the brain does when it’s not doing anything at first, focusing on task-related activity.</p>

<p>Studies have shown that the metabolic demands of the brain do not actually change all that much between resting state and task-related activity, which might mean that there might be some significant amount of background processing going on there. What do you do, when you’re just sitting there staring into space quietly?. According to the <a href="https://internal-journal.frontiersin.org/articles/10.3389/fnhum.2013.00446/full">Amsterdam Resting-state Questionnaire</a>, there are seven dimensions of the resting state: discontinuity of mind (rapidly switching thoughts), theory of mind (a.k.a. mentalizing, fancy terms for “putting yourself in other people’s shoes”), self, planning, sleepiness, comfort and somatic awareness (thinking about the state of your body).</p>

<p>There are interesting correlations of the prevalence of these dimensions with other cognitive tests and clinical variables, which is also a running theme in resting state activity research in general. Providing a total list of biomarkers people have attempted to pull out of the resting state would put anyone to sleep, but resting state activity differences in EEG, MEG and fMRI have been shown in <a href="https://www.sciencedirect.com/science/article/pii/S2352872917300222">neurological disorders</a>, <a href="https://www.frontiersin.org/articles/10.3389/fpsyt.2021.565136/full">mental illnesses</a>, <a href="https://www.sciencedirect.com/science/article/pii/S1053811919305427">brain fingerprinting</a> studies, <a href="https://www.sciencedirect.com/science/article/pii/S0149763413000183">aging</a> and so on.</p>

<p>Methods applied to this activity range from examining power in certain frequency bands, simple correlations between the activity of brain regions to complex graph theory metrics quantifying the relationship between “nodes” in the human brain. <a href="https://direct.mit.edu/netn/article-abstract/3/1/1/2187">Criticism</a> of these methods notwithstanding, resting state activity seems to be a promising and accessible venue of research - no complicated tasks needed, only a valid enough method and 5 to 10 minutes of brain activity recording and we can determine if you are at risk of disease or provide precision treatment (such as transcranial magnetic stimulation, prominent in Parkinson’s disease research, recently used in OCD).</p>

<p>Given how little we still know of the neurobiological underpinnings of mental illnesses in particular, it seems that developing biomarkers of disease and potential treatment response would be a welcome addition to the somewhat outdated tools of using the DSM and trying medications until something sticks. <a href="https://www.nature.com/articles/nm.4246">Some</a> recent papers have shown advances in this field, albeit with potential replication issues. Data-driven approaches, utlizing previously unavailable huge datasets and machine learning models could additionally be <a href="https://onlinelibrary.wiley.com/doi/full/10.1111/pcn.12502#pcn12502-bib-0077">combined</a> with model-based approaches (such as reinforcement learning theories) in computational psychiatry to determine potential loci of pathology/intervention.</p>

<p>However, some <a href="https://journals.library.columbia.edu/index.php/bioethics/article/view/8399">caution</a> is needed. <a href="https://www.sciencedirect.com/science/article/abs/pii/S0277953620303919">Ethics</a> in artificial intelligence has been a growing concern with the increasing prevalence of these methods and indications of potential gender, race and other biases, so ensuring fair and unbiased training data and interpretations is vital. Additionally, the infrastructural cost of precision medicine is something that is not often addressed, such as the costs of keeping terabytes of individualized data, running complex machine learning methods on multiple GPUs and communicating with scientists in the field to ensure state-of-the-art approaches are used.</p>

<p>To close with some food for thought, some interesting observations cast doubt on the content of resting state activity in general, from the paper I started this post with.  Evidence suggests similar structure of resting state activity between wake and sleep, wake and anaesthesia, subjects in a population, and even across mammalian species. This can either mean that resting state activity does not amount to much outside of physiological confounds (e.g. breathing or heart rate artifacts in the data) or, on the contrary, that it captures some universal aspects of brain architecture, processing and cognition.</p>]]></content><author><name></name></author><category term="neuroscience" /><summary type="html"><![CDATA[some thoughts]]></summary></entry></feed>