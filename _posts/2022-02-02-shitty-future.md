---

layout: post
title:  shitty future
date:   2022-02-02 01:15:02
description: i’m not very optimistic
tags: miscellaneous

---

I’m tired of the future. The Twitter account [Shitty Future](https://twitter.com/Shitty_Future) perfectly encapsulates how I feel about the avalanche-like growth of tech (and hype). 

Who doesn’t want a piece of artificial intelligence? It sounds so impressive. Intelligence can be somewhat loosely defined as the capacity for logic, abstract reasoning, decision-making, self-awareness and critical thinking. When you hear “artificial intelligence”, you might even imagine an artificial being possessing these qualities, but here’s where you might be wrong.

### The all-knowing algorithm

What people call AI (or machine learning) can vary. Linear regression is often the first method you look at in machine learning classes. The idea is simple: you assume your data is described by a line and you learn how to draw that line so that it’s a good fit.

Neural networks, while more complex, are also ultimately a powerful but simple idea: instead of a line, you have layers of interconnected units with parameters. If you find the right parameters, this network can be used to predict the output for new data. In practice, one layer of units is not enough to capture the complexity of data, so more layers are added. 

The more complicated the method is, the harder it is to interpret the model it created. When you have a giant network of units that feed signals back and forth predicting complex output (or even [two networks competing with each other to generate output](https://en.wikipedia.org/wiki/Generative_adversarial_network)), can you really tell what your model based its output on? And can you trust its “judgement”?

The allure of saying “an algorithm decided this” seems to be too high, even if the people saying it do not understand the algorithm in the slightest. There is a tendency to [anthropomorphize AI](https://link.springer.com/article/10.1007/s11023-019-09506-6) and even imagine it as an omniscient being - the website [Images of AI](https://blog.betterimagesofai.org/) is dedicated to finding better depictions of AI in the media, ideally those that don’t include pondering robots and a disturbing abundance of the color blue.

<img src="{{ site.baseurl }}/assets/img/ai.png" alt="google img search results for ai" style="zoom:50%;" />

### Do androids dream of anything?

A question that has plagued the field since the infamour Turing test (and possibly before) is - do algorithms actually *understand* what they do? (Aside: I will not talk about the Chinese room thought experiment, mostly because I find thought experiments like that annoying).

However, there have been interesting developments in this question. In particular, [this article by Melanie Mitchell](https://www.quantamagazine.org/what-does-it-mean-for-ai-to-understand-20211216/) talks about GPT-3, a famous language generation model that scores suspiciously high on questions that test understanding on ambiguous sentences (example: *I poured water from the bottle into the cup until it was full. What was full, the bottle or the cup?*). A lot of knowledge and assumptions that we take for granted are simply absent from models, which in turns influences how they behave.

### Time is a flat circle

But, to come back to the less philosophical topic at hand: the humans behind the AI. There is no new thing under the sun, and if someone invented physiognomics or eugenics once, they will be unfortunately invented again.

In the field of AI, common sense had not deterred people from: [detecting someone’s profession from their photo](https://venturebeat.com/2021/01/11/outlandish-stanford-facial-recognition-study-claims-there-are-links-between-facial-features-and-political-orientation/), [detecting sexual orientation from pictures](https://www.gsb.stanford.edu/faculty-research/publications/deep-neural-networks-are-more-accurate-humans-detecting-sexual), [digitally undressing people](https://www.huffpost.com/entry/deepfake-tool-nudify-women_n_6112d765e4b005ed49053822), [generating faces of people from their speech](https://speech2face.github.io/), [putting people’s faces in videos (including pretty compromising ones)](https://en.wikipedia.org/wiki/Deepfake), [recognizing people’s ethnicity from their faces](https://www.bbc.com/news/world-australia-58571618), [including people’s photos in databases without their consent](https://www.technologyreview.com/2021/08/13/1031836/ai-ethics-responsible-data-stewardship/), and [predicting prison sentences from legal cases](https://aclanthology.org/D19-1667/) (some researchers, like the author of the sexuality detection paper, even had the audacity to claim that they are exposing threats to the LGBT community instead of, clearly, being a threat themselves).

An excellent [paper by Birhane et al, 2021](https://arxiv.org/pdf/2106.15590.pdf) identifies implicit values that are upheld within machine learning research. Perhaps unsurprisingly, “performance”, “novelty” and “generalization” are at the forefront, while “useful”, “easy to implement” and “interpretable” are somewhere in the middle. “Fairness”, “justice” and other similar concepts have been relegated to the end. While it might seem alarmist to call an entire field completely oblivious to fairness and bias, the ever-growing kaiju battle/arms race of gigantic models to outperform each other on some arbitrary benchmark does seem concerning.

### Extra credit

If you want to feel even less optimistic about the future, check out [Web3 is doing great!](https://web3isgoinggreat.com/), an astonishing collage of incompetence, malice and stupidity of NFT and crypto-bros (the Venn diagram of people into NFTs and those who think AI will take over the world is probably a circle).

If you dislike a certain billionaire as much as I do, check out a very cool [Twitter thread](https://twitter.com/tdverstynen/status/1485321117065695243) taking apart his latest nonsense.





